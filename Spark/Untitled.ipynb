{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fe6ac8be5d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#df=pd.read_csv('/opt/fichiers/projet/version2_0.csv', delimiter=\";\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"nom\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prenom\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"adresse\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ville\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"region\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nb_identite\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdfj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Documents/projet/yourjson.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Documents/projet/yourjson.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, getopt, pprint\n",
    "from pymongo import MongoClient\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "#df=pd.read_csv('/opt/fichiers/projet/version2_0.csv', delimiter=\";\")\n",
    "data = DataFrame(df, columns = [\"nom\",\"prenom\",\"adresse\",\"ville\",\"region\",\"nb_identite\"])\n",
    "dfj=df.to_json('Documents/projet/yourjson.json',orient='table')\n",
    "jdf = open('Documents/projet/yourjson.json').read()\n",
    "data = json.loads(jdf)\n",
    "\n",
    "client = MongoClient('localhost',27017)\n",
    "db = client.db\n",
    "collection = db.collection\n",
    "\n",
    "\n",
    "db.collection.insert_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---------+--------+-------------+------------+\n",
      "|                 _id|             adresse|nb_identite|      nom|  prenom|       region|       ville|\n",
      "+--------------------+--------------------+-----------+---------+--------+-------------+------------+\n",
      "|[5ecd03454a71cb0c...|    East Main Street|    1437018|  Johnson| Millard|      Vermont|       Dover|\n",
      "|[5ecd03454a71cb0c...|       Richmond Hill|     119595|  Harding|  Gerald|      Florida| Little Rock|\n",
      "|[5ecd03454a71cb0c...|      Cleveland Ave.|    2303298|  Johnson|  Gerald|       Kansas|  Harrisburg|\n",
      "|[5ecd03454a71cb0c...|   San Diego Freeway|    8070934|  Clinton|  Thomas|     Missouri|   Frankfort|\n",
      "|[5ecd03454a71cb0c...|        Camelback Rd|    3255890|    Hayes|  Thomas|   New Jersey|      Boston|\n",
      "|[5ecd03454a71cb0c...|    East Main Street|    1549796| Harrison|  Grover|        Maine|     Atlanta|\n",
      "|[5ecd03454a71cb0c...|      Cleveland Ave.|    3875962| Harrison|  Warren|      Vermont|     Jackson|\n",
      "|[5ecd03454a71cb0c...|    Santa Rosa South|    1309776|Cleveland| Chester|     Michigan|       Dover|\n",
      "|[5ecd03454a71cb0c...|   Carpinteria South|    7469709|     Polk|  Andrew|   New Mexico|      Boston|\n",
      "|[5ecd03454a71cb0c...|South Roosevelt D...|    8363427|    Hayes|  Warren|       Nevada|     Jackson|\n",
      "|[5ecd03454a71cb0c...|    Cabrillo Highway|    9791185|  Harding|  Warren|     Nebraska|     Madison|\n",
      "|[5ecd03454a71cb0c...|          San Marcos|    8178513|  Kennedy|Theodore|     New York|      Boston|\n",
      "|[5ecd03454a71cb0c...|    East Main Street|    8279235|   Reagan|  Andrew|     Delaware| Tallahassee|\n",
      "|[5ecd03454a71cb0c...|    Bayshore Freeway|    5127682|    Adams|    Bill|New Hampshire|       Salem|\n",
      "|[5ecd03454a71cb0c...|     East 1st Street|    9084561| Harrison| Chester| South Dakota|Indianapolis|\n",
      "|[5ecd03454a71cb0c...|            Via Real|    5334883| Buchanan|    John|       Hawaii|     Olympia|\n",
      "|[5ecd03454a71cb0c...|        Newbury Road|    6344370|  Johnson|Franklin|New Hampshire|     Atlanta|\n",
      "|[5ecd03454a71cb0c...|         S Rustle St|    7963219|   Quincy|Theodore|     Kentucky|    Hartford|\n",
      "|[5ecd03454a71cb0c...|    Bayshore Freeway|    2772928| Fillmore| Richard|New Hampshire|  Des Moines|\n",
      "|[5ecd03454a71cb0c...|          Jones Road|    7341034|   Hoover|Benjamin|      Vermont|     Jackson|\n",
      "+--------------------+--------------------+-----------+---------+--------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader, DataFrameReader\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "\n",
    "spark = SparkSession.builder.appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/malade.contamine?readPreference=primaryPreferred\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/malade.contamine\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_csv = spark.read.csv(\"/opt/fichiers/projet/version2_0.csv\", header=True,mode=\"DROPMALFORMED\",sep=\";\")\n",
    "df_csv.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('append').option('database',\"malade\").option('collection',\"contamine\").save()\n",
    "df01 = spark.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"malade\")\\\n",
    "    .option(\"collection\", \"contamine\")\\\n",
    "    .load()\n",
    "\n",
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+--------+------------+--------------+--------------------+-------------+---------------+-----------+---------+--------+------------+-----------+--------------------+-------------+--------------+--------+\n",
      "|           Adresse_C|Id_contamine_C|     Nom_C|Prenom_C|    Region_C|       Ville_C|                 _id|nb_identite_C|      Adresse_P|Id_personne|    Nom_P|Prenom_P|    Region_P|    Ville_P|                 _id|nb_identite_P|date_rencontre|distance|\n",
      "+--------------------+--------------+----------+--------+------------+--------------+--------------------+-------------+---------------+-----------+---------+--------+------------+-----------+--------------------+-------------+--------------+--------+\n",
      "|San Luis Obispo N...|             9|Eisenhower|  Ronald|South Dakota|Salt Lake City|[5edeea534a71cb23...|       214334|E Fowler Avenue|          6|Jefferson| Zachary|Rhode Island|Carson City|[5edeea534a71cb23...|        31775|         22/02|       1|\n",
      "|         Tanger Blvd|            10|      Taft|Franklin|    Delaware|         Boise|[5edeea534a71cb23...|       136274|E Fowler Avenue|          6|Jefferson| Zachary|Rhode Island|Carson City|[5edeea534a71cb23...|        31775|         22/02|       1|\n",
      "|San Luis Obispo N...|             9|Eisenhower|  Ronald|South Dakota|Salt Lake City|[5edeea534a71cb23...|       214334|      Harbor Dr|          5|   Pierce| Herbert|   Minnesota|      Boise|[5edeea534a71cb23...|        37002|         21/02|       0|\n",
      "|         Tanger Blvd|            10|      Taft|Franklin|    Delaware|         Boise|[5edeea534a71cb23...|       136274|      Harbor Dr|          5|   Pierce| Herbert|   Minnesota|      Boise|[5edeea534a71cb23...|        37002|         21/02|       1|\n",
      "|San Luis Obispo N...|             9|Eisenhower|  Ronald|South Dakota|Salt Lake City|[5edeea534a71cb23...|       214334| Katella Avenue|          3|    Grant|Franklin|       Maine|    Raleigh|[5edeea534a71cb23...|        50637|         21/02|       1|\n",
      "|         Tanger Blvd|            10|      Taft|Franklin|    Delaware|         Boise|[5edeea534a71cb23...|       136274| Katella Avenue|          3|    Grant|Franklin|       Maine|    Raleigh|[5edeea534a71cb23...|        50637|         21/02|       1|\n",
      "|San Luis Obispo N...|             9|Eisenhower|  Ronald|South Dakota|Salt Lake City|[5edeea534a71cb23...|       214334| East Fry Blvd.|          4|     Taft|  Calvin|    New York|   Hartford|[5edeea534a71cb23...|        60782|         22/02|       0|\n",
      "|         Tanger Blvd|            10|      Taft|Franklin|    Delaware|         Boise|[5edeea534a71cb23...|       136274| East Fry Blvd.|          4|     Taft|  Calvin|    New York|   Hartford|[5edeea534a71cb23...|        60782|         22/02|       0|\n",
      "+--------------------+--------------+----------+--------+------------+--------------+--------------------+-------------+---------------+-----------+---------+--------+------------+-----------+--------------------+-------------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# first connector to connect spark and mongo\n",
    "spark_malade = SparkSession.builder.appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/population.coronavirus?readPreference=primaryPreferred\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/population.coronavirus\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# second connector to connect spark and mongo\n",
    "spark_contamine = SparkSession.builder.appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/population.contamine?readPreference=primaryPreferred\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/population.contamine\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# third connector to connect spark and mongo to make SQL request\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#spark read csv file\n",
    "df_csv_malade = spark_malade.read.csv(\"/opt/fichiers/projet/malade.csv\", header=True,mode=\"DROPMALFORMED\",sep=\";\")\n",
    "\n",
    "#spark send dataframe to mongodb\n",
    "df_csv_malade.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('overwrite').option('database',\"population\").option('collection',\"coronavirus\").save()\n",
    "\n",
    "#spark read second csv file\n",
    "df_csv_contamine = spark_contamine.read.csv(\"/opt/fichiers/projet/contamine.csv\", header=True,mode=\"DROPMALFORMED\",sep=\";\")\n",
    "\n",
    "#spark send dataframe to mongodb\n",
    "df_csv_contamine.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('overwrite').option('database',\"population\").option('collection',\"contamine\").save()\n",
    "\n",
    "\n",
    "df01 = spark_malade.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"population\")\\\n",
    "    .option(\"collection\", \"coronavirus\")\\\n",
    "    .load()\n",
    "\n",
    "df02 = spark_contamine.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"population\")\\\n",
    "    .option(\"collection\", \"contamine\")\\\n",
    "    .load()\n",
    "\n",
    "#cross 2 dfs\n",
    "df_final = df02.crossJoin(df01)\n",
    "\n",
    "#list of tuple and convert to rdd\n",
    "date_rencontre =[(9,1,1,\"21/02\",3),(9,2,2,\"22/02\",2),(9,3,3,\"21/02\",1),(9,4,4,\"22/02\",0),(9,5,5,\"21/02\",0),(9,6,6,\"22/02\",1),(9,7,7,\"21/02\",2),(9,8,8,\"22/02\",3),(10,1,9,\"21/02\",3),(10,2,10,\"22/02\",2),(10,3,11,\"21/02\",1),(10,4,12,\"22/02\",0),(10,5,13,\"21/02\",1),(10,6,14,\"22/02\",1),(10,7,15,\"21/02\",2),(10,8,16,\"22/02\",3)]\n",
    "rdd_date = spark.sparkContext.parallelize(date_rencontre)\n",
    "\n",
    "#rdd to map to df\n",
    "map_date = rdd_date.map(lambda x:Row(Id_contamine_C=x[0],Id_personne =x[1],id_date=x[2],date_rencontre=x[3],distance=x[4]))\n",
    "df_date = spark.createDataFrame(map_date)\n",
    "   \n",
    "#create 2 temps views to do sql request\n",
    "df_final.createOrReplaceTempView(\"sortie\")\n",
    "df_date.createOrReplaceTempView(\"date\")\n",
    "\n",
    "#sql request\n",
    "request_sec = spark.sql(\"SELECT sortie.*,date_rencontre,distance FROM sortie inner join date on sortie.Id_personne = date.Id_personne and sortie.Id_contamine_C = date.Id_contamine_C Where distance <= 1\")\n",
    "\n",
    "#show me result\n",
    "request_sec.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
